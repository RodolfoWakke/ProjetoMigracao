<transformation>
  <info>
    <name>APAGAR</name>
    <description/>
    <extended_description/>
    <trans_version/>
    <trans_type>Normal</trans_type>
    <trans_status>0</trans_status>
    <directory>/</directory>
    <parameters>
    </parameters>
    <log>
      <trans-log-table>
        <connection/>
        <schema/>
        <table/>
        <size_limit_lines/>
        <interval/>
        <timeout_days/>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STATUS</id>
          <enabled>Y</enabled>
          <name>STATUS</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
          <subject/>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
          <subject/>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
          <subject/>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
          <subject/>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
          <subject/>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
          <subject/>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>STARTDATE</id>
          <enabled>Y</enabled>
          <name>STARTDATE</name>
        </field>
        <field>
          <id>ENDDATE</id>
          <enabled>Y</enabled>
          <name>ENDDATE</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>DEPDATE</id>
          <enabled>Y</enabled>
          <name>DEPDATE</name>
        </field>
        <field>
          <id>REPLAYDATE</id>
          <enabled>Y</enabled>
          <name>REPLAYDATE</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>Y</enabled>
          <name>LOG_FIELD</name>
        </field>
        <field>
          <id>EXECUTING_SERVER</id>
          <enabled>N</enabled>
          <name>EXECUTING_SERVER</name>
        </field>
        <field>
          <id>EXECUTING_USER</id>
          <enabled>N</enabled>
          <name>EXECUTING_USER</name>
        </field>
        <field>
          <id>CLIENT</id>
          <enabled>N</enabled>
          <name>CLIENT</name>
        </field>
      </trans-log-table>
      <perf-log-table>
        <connection/>
        <schema/>
        <table/>
        <interval/>
        <timeout_days/>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>SEQ_NR</id>
          <enabled>Y</enabled>
          <name>SEQ_NR</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>INPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>INPUT_BUFFER_ROWS</name>
        </field>
        <field>
          <id>OUTPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>OUTPUT_BUFFER_ROWS</name>
        </field>
      </perf-log-table>
      <channel-log-table>
        <connection/>
        <schema/>
        <table/>
        <timeout_days/>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>LOGGING_OBJECT_TYPE</id>
          <enabled>Y</enabled>
          <name>LOGGING_OBJECT_TYPE</name>
        </field>
        <field>
          <id>OBJECT_NAME</id>
          <enabled>Y</enabled>
          <name>OBJECT_NAME</name>
        </field>
        <field>
          <id>OBJECT_COPY</id>
          <enabled>Y</enabled>
          <name>OBJECT_COPY</name>
        </field>
        <field>
          <id>REPOSITORY_DIRECTORY</id>
          <enabled>Y</enabled>
          <name>REPOSITORY_DIRECTORY</name>
        </field>
        <field>
          <id>FILENAME</id>
          <enabled>Y</enabled>
          <name>FILENAME</name>
        </field>
        <field>
          <id>OBJECT_ID</id>
          <enabled>Y</enabled>
          <name>OBJECT_ID</name>
        </field>
        <field>
          <id>OBJECT_REVISION</id>
          <enabled>Y</enabled>
          <name>OBJECT_REVISION</name>
        </field>
        <field>
          <id>PARENT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>PARENT_CHANNEL_ID</name>
        </field>
        <field>
          <id>ROOT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>ROOT_CHANNEL_ID</name>
        </field>
      </channel-log-table>
      <step-log-table>
        <connection/>
        <schema/>
        <table/>
        <timeout_days/>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>N</enabled>
          <name>LOG_FIELD</name>
        </field>
      </step-log-table>
      <metrics-log-table>
        <connection/>
        <schema/>
        <table/>
        <timeout_days/>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>METRICS_DATE</id>
          <enabled>Y</enabled>
          <name>METRICS_DATE</name>
        </field>
        <field>
          <id>METRICS_CODE</id>
          <enabled>Y</enabled>
          <name>METRICS_CODE</name>
        </field>
        <field>
          <id>METRICS_DESCRIPTION</id>
          <enabled>Y</enabled>
          <name>METRICS_DESCRIPTION</name>
        </field>
        <field>
          <id>METRICS_SUBJECT</id>
          <enabled>Y</enabled>
          <name>METRICS_SUBJECT</name>
        </field>
        <field>
          <id>METRICS_TYPE</id>
          <enabled>Y</enabled>
          <name>METRICS_TYPE</name>
        </field>
        <field>
          <id>METRICS_VALUE</id>
          <enabled>Y</enabled>
          <name>METRICS_VALUE</name>
        </field>
      </metrics-log-table>
    </log>
    <maxdate>
      <connection/>
      <table/>
      <field/>
      <offset>0.0</offset>
      <maxdiff>0.0</maxdiff>
    </maxdate>
    <size_rowset>30000</size_rowset>
    <sleep_time_empty>50</sleep_time_empty>
    <sleep_time_full>50</sleep_time_full>
    <unique_connections>N</unique_connections>
    <feedback_shown>Y</feedback_shown>
    <feedback_size>60000</feedback_size>
    <using_thread_priorities>Y</using_thread_priorities>
    <shared_objects_file/>
    <capture_step_performance>N</capture_step_performance>
    <step_performance_capturing_delay>1000</step_performance_capturing_delay>
    <step_performance_capturing_size_limit>100</step_performance_capturing_size_limit>
    <dependencies>
    </dependencies>
    <partitionschemas>
    </partitionschemas>
    <slaveservers>
    </slaveservers>
    <clusterschemas>
    </clusterschemas>
    <created_user>-</created_user>
    <created_date>2023/05/05 15:33:04.450</created_date>
    <modified_user>-</modified_user>
    <modified_date>2024/02/16 14:18:55.430</modified_date>
    <key_for_session_key>H4sIAAAAAAAAAAMAAAAAAAAAAAA=</key_for_session_key>
    <is_key_private>N</is_key_private>
  </info>
  <notepads>
    <notepad>
      <note>PASSO 1: Esse tranformation é o passo inicial da migração. Como o backup é enviado em excel, neste passo nós inserimos a tabela Xlsx no DB.
Logo, no início, antes de rodar o Spoon, é necessário criar o DB no servidor do serve-pc, depois, já pelo Spoon, criar as tabelas antes de inserir os registros.</note>
      <xloc>5</xloc>
      <yloc>-84</yloc>
      <width>1240</width>
      <heigth>58</heigth>
      <fontname>Segoe UI</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <order>
  </order>
  <step>
    <name>Dummy (do nothing)</name>
    <type>Dummy</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name/>
    </partitioning>
    <attributes/>
    <cluster_schema/>
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>272</xloc>
      <yloc>512</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Dummy (do nothing) 2</name>
    <type>Dummy</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name/>
    </partitioning>
    <attributes/>
    <cluster_schema/>
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>416</xloc>
      <yloc>448</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>CPython Script Executor</name>
    <type>CPythonScriptExecutor</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name/>
    </partitioning>
    <rows_to_process>ALL</rows_to_process>
    <rows_to_process_size/>
    <reservoir_sampling>N</reservoir_sampling>
    <reservoir_sampling_size/>
    <reservoir_sampling_seed>1</reservoir_sampling_seed>
    <include_input_as_output>N</include_input_as_output>
    <py_script>import os
import pandas as pd
import mysql.connector
from mysql.connector import Error
from tqdm import tqdm
from colorama import Fore, Style, Back


 #PROGRAMA QUE FAZ INSERÇÃO DOS DADOS DE .CSV EM UMA TABELA DO BANCO DE DADOS
 #ALTERE A VARIÁVEL 'pasta_csv' PARA O CAMINHO DA PASTA QUE CONTÉM OS ARQUIVOS CSV
 #ALTERE AS CONFIGURAÇÕES DO BANCO DE DADOS CONFORME NECESSÁRIO


# Configurações do banco de dados
db_config = {
    'user': 'root',
    'password': '123456',
    'host': 'localhost',
    'database':  '${v_nomebanco-mysql}', # &lt;█████████ ALTERE O NOME DO BANCO DE DADOS █████████ #
    'connection_timeout': 3600,  
}

def msg(m):
    tam = len(m) + 8
    print('=' * tam)
    print(f'{m}')
    print('=' * tam)

# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Pasta contendo os arquivos CSV
pasta_csv = r'${v_diretorio-backup}'

# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

print('Inserção de arquivos CSV no banco de dados...')
msg(f"Servidor: {Fore.YELLOW}{db_config['host']}{Style.RESET_ALL}\nBanco de dados: {Fore.YELLOW}{db_config['database']}{Style.RESET_ALL}\nArquivo: {Fore.YELLOW}{pasta_csv}{Style.RESET_ALL}")

def criar_tabela(cursor, nome_tabela, colunas):
    # Criar a tabela no banco de dados
    query = f"CREATE TABLE IF NOT EXISTS {nome_tabela} ({', '.join(colunas)}"

    # Adicionar índices para colunas que se repetem com o prefixo 'id' e 'cod'
    index_columns = [col for col in df.columns if col.lower().startswith(("id", "cod")) or col.lower().endswith(("id", "cod"))]

    # Limitar o número de índices a 64
    for i, col in enumerate(index_columns):
        if i >= 64:
            break

        # Truncate index name if it's too long
        index_name = f"idx_{nome_tabela}_{col}"[:64]  # 64 characters is a common limit
        query += f", INDEX {index_name} ({col})"

    query += ") ENGINE=InnoDB ROW_FORMAT=DYNAMIC"

    cursor.execute(query)


def inserir_dados(cursor, nome_tabela, dados):
    # Inserir dados na tabela
    query = f"INSERT INTO {nome_tabela} VALUES ({', '.join(['%s']*len(dados))})"
    cursor.execute(query, dados)

# Conectar ao banco de dados
try:
    connection = mysql.connector.connect(**db_config)

    if connection.is_connected():
        cursor = connection.cursor()

        # Variáveis de contagem
        tabelas_lidas = 0
        tabelas_existentes_na_pasta = 0
        tabelas_inseridas = 0

        # Iterar sobre os arquivos CSV na pasta com tqdm
        for arquivo_csv in tqdm(os.listdir(pasta_csv), desc='Processando arquivos', unit='arquivo'):
            if arquivo_csv.endswith(".csv"):
                caminho_arquivo = os.path.join(pasta_csv, arquivo_csv)

                # Incrementar a contagem de tabelas existentes na pasta
                tabelas_existentes_na_pasta += 1

                # Verificar se o arquivo não está vazio
                if os.path.getsize(caminho_arquivo) == 0:
                    nome_arquivo = os.path.basename(caminho_arquivo)
                    print(f"Arquivo vazio: {nome_arquivo}")
                    continue

                # Ler o arquivo CSV usando pandas
                try:
                    df = pd.read_csv(caminho_arquivo, dtype=str, encoding='UTF-8', sep=';') #ISO-8859-1

                    # Tratar valores NaN substituindo por None
                    df = df.where(pd.notna(df), None)

                    # Obter o nome da tabela e colunas
                    nome_tabela = os.path.splitext(arquivo_csv)[0]

                    # Verificar se a tabela já existe no banco de dados
                    cursor.execute(f"SHOW TABLES LIKE '{nome_tabela}'")
                    tabela_existe = cursor.fetchone()

                    if tabela_existe:
                        print(f"Tabela já existe para o arquivo: {caminho_arquivo}")
                        continue  # Pular para o próximo arquivo

                    # Criar a tabela no banco de dados
                    colunas = []

                    # Lista de colunas que devem ser do tipo INT
                    int_columns = []

                    for col in df.columns:
                        # Verificar se todos os valores na coluna podem ser convertidos para int
                        try:
                            if df[col].dropna().astype(float).apply(lambda x: x.is_integer() if not pd.isna(x) else True).all():
                                int_columns.append(col)
                        except ValueError:
                            pass  # Se houver erro ao converter para float, assume que não é uma coluna do tipo INT

                    for col in df.columns:
                        # Se a coluna estiver na lista de colunas do tipo INT
                        if col in int_columns:
                            colunas.append(f"{col} INT")
                        else:
                            # Determinar o tipo de coluna com base no máximo do comprimento
                            max_length = df[col].astype(str).apply(len).max()
                            tipo_coluna = f"{col} VARCHAR({max_length})" if max_length &lt;= 500 else f"{col} TEXT"
                            colunas.append(tipo_coluna)


                    criar_tabela(cursor, nome_tabela, colunas)

                    # Incrementar a contagem de tabelas inseridas
                    tabelas_inseridas += 1

                    # Inserir os dados na tabela
                    for _, linha in df.iterrows():
                        # Substituir valores NaN por None antes da inserção
                        linha = tuple(None if pd.isna(valor) else valor for valor in linha)
                        inserir_dados(cursor, nome_tabela, linha)
                except pd.errors.EmptyDataError:
                    nome_arquivo = os.path.basename(caminho_arquivo)
                    print(f"\nErro ao ler o arquivo: {nome_arquivo}. Arquivo vazio ou não existe.")
                except UnicodeDecodeError:
                    print(f"Erro de codificação ao ler o arquivo: {caminho_arquivo}. Tente especificar outro encoding.")
                except Exception as e:

                    df = pd.read_csv(caminho_arquivo, dtype=str, encoding='UTF-8', sep=',') #ISO-8859-1

                    # Tratar valores NaN substituindo por None
                    df = df.where(pd.notna(df), None)

                    # Obter o nome da tabela e colunas
                    nome_tabela = os.path.splitext(arquivo_csv)[0]

                    # Verificar se a tabela já existe no banco de dados
                    cursor.execute(f"SHOW TABLES LIKE '{nome_tabela}'")
                    tabela_existe = cursor.fetchone()

                    if tabela_existe:
                        print(f"Tabela já existe para o arquivo: {caminho_arquivo}")
                        continue  # Pular para o próximo arquivo

                    # Criar a tabela no banco de dados
                    colunas = []

                    # Lista de colunas que devem ser do tipo INT
                    int_columns = []

                    for col in df.columns:
                        # Verificar se todos os valores na coluna podem ser convertidos para int
                        try:
                            if df[col].dropna().astype(float).apply(lambda x: x.is_integer() if not pd.isna(x) else True).all():
                                int_columns.append(col)
                        except ValueError:
                            pass  # Se houver erro ao converter para float, assume que não é uma coluna do tipo INT

                    for col in df.columns:
                        # Se a coluna estiver na lista de colunas do tipo INT
                        if col in int_columns:
                            colunas.append(f"{col} INT")
                        else:
                            # Determinar o tipo de coluna com base no máximo do comprimento
                            max_length = df[col].astype(str).apply(len).max()
                            tipo_coluna = f"{col} VARCHAR({max_length})" if max_length &lt;= 500 else f"{col} TEXT"
                            colunas.append(tipo_coluna)

                    criar_tabela(cursor, nome_tabela, colunas)

                    # Incrementar a contagem de tabelas inseridas
                    tabelas_inseridas += 1

                    # Inserir os dados na tabela
                    for _, linha in df.iterrows():
                        # Substituir valores NaN por None antes da inserção
                        linha = tuple(None if pd.isna(valor) else valor for valor in linha)
                        inserir_dados(cursor, nome_tabela, linha)

        # Commit e fechar a conexão
        connection.commit()
        cursor.close()

except Error as e:
    print(f"Erro: {e}")

finally:
    if connection.is_connected():
        connection.close()
        print("Conexão encerrada.")</py_script>
    <load_script_at_runtime>N</load_script_at_runtime>
    <script_to_load/>
    <continue_on_unset_vars>Y</continue_on_unset_vars>
    <py_vars_to_get/>
    <include_frame_row_index>N</include_frame_row_index>
    <frame_names>
    </frame_names>
    <incoming_step_names>
   </incoming_step_names>
    <attributes/>
    <cluster_schema/>
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>432</xloc>
      <yloc>176</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step_error_handling>
  </step_error_handling>
  <slave-step-copy-partition-distribution>
  </slave-step-copy-partition-distribution>
  <slave_transformation>N</slave_transformation>
  <attributes/>
</transformation>
